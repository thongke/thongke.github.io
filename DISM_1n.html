<h1 id="what-and-why-spatial-statistics"><span class="header-section-number">1</span> What and why spatial statistics?</h1><h2 id="spatial-epidemiology"><span class="header-section-number">1.1</span> Spatial Epidemiology</h2><p>Concerns the analysis of the spatial/geographical distribution of the incidence or prevalence of disease. Also called:</p><ul><li>geographical epidemiology</li><li>environmental epidemiology</li><li>medical geography</li><li>small area health study</li><li>spatial biostatistics</li></ul><blockquote><p>How to analyse geographical data when we have geographical information available?</p></blockquote><h3 id="non-spatial-analysis"><span class="header-section-number">1.1.1</span> Non-Spatial Analysis</h3><ul><li>Using conventional statistical methods</li><li>The results are independent of the spatial arrangement of the geographical entities</li><li>Observations or entities are assumed to be independent and identically distributed, or in some occasions temporal dependence are also explored.</li></ul><table><thead><tr class="header"><th align="left"></th><th align="left">Variable 1</th><th align="left">Variable 2</th><th align="left">Variable n</th></tr></thead><tbody><tr class="odd"><td align="left">Entity 1</td><td align="left">attribute<sub>1</sub></td><td align="left">attribute<sub>12</sub></td><td align="left">attribute<sub>1n</sub></td></tr><tr class="even"><td align="left">Entity 2</td><td align="left">attribute<sub>21</sub></td><td align="left">attribute<sub>22</sub></td><td align="left">attribute<sub>2n</sub></td></tr><tr class="odd"><td align="left">Entity m</td><td align="left">attribute<sub>m1</sub></td><td align="left">attribute<sub>m2</sub></td><td align="left">attribute<sub>mn</sub></td></tr></tbody></table><h3 id="spatial-analysis"><span class="header-section-number">1.1.2</span> Spatial analysis</h3><ul><li>When geographical information available, data are called geo-referenced</li><li>Use of spatial statistical methods</li><li>The results depend on the spatial arrangement of the geographical entities</li><li>It can also include temporal dependence</li></ul><table><thead><tr class="header"><th align="left"></th><th align="left">Geographical</th><th align="left"></th><th align="left">attribute</th><th align="left"></th><th align="left"></th></tr></thead><tbody><tr class="odd"><td align="left"></td><td align="left">X</td><td align="left">Y</td><td align="left">Variable 1</td><td align="left">Variable 2</td><td align="left">Variable n</td></tr><tr class="even"><td align="left">Entity 1</td><td align="left">X<sub>1</sub></td><td align="left">Y1</td><td align="left">attribute<sub>1</sub></td><td align="left">attribute<sub>12</sub></td><td align="left">attribute<sub>1n</sub></td></tr><tr class="odd"><td align="left">Entity 2</td><td align="left">X<sub>2</sub></td><td align="left">Yg</td><td align="left">attribute<sub>21</sub></td><td align="left">attribute<sub>22</sub></td><td align="left">attribute<sub>2n</sub></td></tr><tr class="even"><td align="left">Entity m</td><td align="left">X<sub>m</sub></td><td align="left">Ym</td><td align="left">attribute<sub>m1</sub></td><td align="left">attribute<sub>m2</sub></td><td align="left">attribute<sub>mn</sub></td></tr></tbody></table><h3 id="areas-of-applications"><span class="header-section-number">1.1.3</span> Areas of Applications</h3><p>Earliest example of spatial epidemiology: Dr. John Snow’s study of spatial distribution of cholera victims. Epidemic around the Broad Street pump in London (1854)</p><p>Other example: In an epidemiological investigation, we might wish to analyze lung, breast, colorectal, and cervical cancer rates by county and year in a particular state, with smoking, mammography, and other important screening and staging information also available at some level.</p><div class="figure"><img src="a1.png" alt="Source: Limburg cancer registry data (Belgium)" /><p class="caption">Source: Limburg cancer registry data (Belgium)</p></div><p>Researchers in diverse areas such as climatology, ecology, environmental health, and real estate marketing are increasingly faced with the task of analyzing data that are highly multivariate, with many important predictors and response variables, geographically referenced, and often presented as maps and temporally correlated, as in longitudinal or other time series structures.</p><h3 id="statistical-inference"><span class="header-section-number">1.1.4</span> Statistical Inference</h3><p>Public health professionals who collect such data are charged not only with surveillance, but also statistical inference tasks, such as <strong>modeling</strong> of trends and correlation structures, <strong>estimation</strong> of underlying model parameters, <strong>hypothesis testing</strong> (or comparison of competing models), <strong>prediction</strong> of observations at unobserved times or locations. All naturally accomplished through hierarchical modeling (which can be implemented via Markov chain Monte Carlo (MCMC) methods or other frequentist approaches)</p><h3 id="spatial-statistics-books"><span class="header-section-number">1.1.5</span> Spatial Statistics Books</h3><p>Pioneer Book: Paelinck and Klaassen (1979): which focused the attention of regional scientists on the need for specialized econometric methods to deal with estimation and specification problems caused by spatial data.</p><p>Books in the field: Cressie (1990, 1993): the legendary “bible” of spatial statistics, but rather high mathematical level and lacks modern hierarchical modeling/computing. Wackernagel (1998): terse; only geostatistics. Chiles and Delfiner (1999): only geostatistics. Stein (1999a): theoretical treatise on kriging.</p><p>More descriptive presentations: Bailey and Gatrell (1996) focuses on description of pattern, tests of hypotheses, and interpolation, the authors continually stress the role of visualization in understanding spatial phenomena. Fotheringham and Rogerson (1994): deal with the integration of GIS and spatial analysis. Haining (1990):include data description, map interpolation, exploratory and explanatory analyses.</p><p>More recent books: Banerjee, S., Carlin, B.P. and Gelfand, A.E. (2004) Hierarchical modeling of analysis for spatial data, CRC Press. Waller, LA, and Gotway, CA. (2004) Applied spatial statistics for public health, Wiley &amp; Sons. Bivand, R.S., Pebesma, E.J., Gomez-Rubio, V. (2008) Applied Spatial Data Analysis with R, Springer. Lawson, A. (2009) Bayesian disease mapping. Hierarchical modeling in spatial epidemiology, Chapman &amp; Hall. Gelfand, A.E., Diggle, P., Fuentes, M. and Guttorp, P. (2011) Handbook of Spatial Statistics, CRC press. Cressie, N. and Wikle, C. (2011) Statistics for spatio-temporal data.</p><h2 id="properties-and-nature-of-spatial-data-and-spatial-process"><span class="header-section-number">1.2</span> Properties and Nature of Spatial Data and Spatial Process</h2><p>What is Special about Spatial Data?</p><ul><li>Location, Location, Location: where matters</li><li>Spatial Dependence is the rule<ul><li>spatial interaction, contagion, externalities, spill-overs, copycatting</li><li>The first law of geography (Tobler): everything depends on everything else, but closer things more.</li></ul></li><li>Spatial Heterogeneity (or Non-stationarity)<ul><li>The second law of geography (a law of spatial heterogeneity): conditions vary (“smoothly”) over the Earth’s surface</li></ul></li><li>Pertains to the spatial or regional differentiation observed in the value of a variable: Spatial drift (e.g., a trend surface), spatial association.</li><li>The properties of geographical data present a fundamental challenge to classical (conventional) statistics.</li><li>They violate classical assumptions of independence and homogeneity (stationarity) and render classical methods inefficient or inappropriate.</li></ul><h3 id="nature-of-spatial-data"><span class="header-section-number">1.2.1</span> Nature of Spatial Data</h3><p>Spatially referenced data “georeferenced”: “attribute” data associated with location: <strong>where matters</strong>. Example, spatial Objects</p><ul><li>points: x, y coordinates =&gt; cities, stores, crimes, accidents</li><li>lines: arcs, from node, to node =&gt; road network, transmission lines</li><li>polygons: series of connected arcs =&gt; provinces, cities, census tracts</li></ul><div class="figure"><img src="a2.png" /></div><div class="figure"><img src="a3.png" /></div><h3 id="spatial-process-basic-definitions"><span class="header-section-number">1.2.2</span> Spatial Process: Basic Definitions</h3><h4 id="regionalized-variable"><span class="header-section-number">1.2.2.1</span> Regionalized Variable</h4><p>Any variable distributed in space is said to be “regionalized” or “spatial”. Some examples of regionalized variables are:</p><ol style="list-style-type: decimal"><li>The price of gold on the NYSE in time (one dimension - longitudinal data)</li><li>Total snowfall during January in the province Luxembourg (two dimensions)</li><li>Magnesium content in the soil (in ppm), measured at varying depths in an agricultural field (three dimensions)</li></ol><p>We view a regionalized variable as a function <span class="math">\(f(\mathbf{s})\)</span> which adopts a value at every point <em>s</em> in the appropriately defined space. The “site” or “location” <strong>s<sub>i</sub></strong> is bold to indicate that it may be multidimensional. For example, over some field, we might view <strong>s<sub>i</sub></strong> as (<span class="math">\(x_{1i},x_{2i}\)</span>), where <span class="math">\(x_1, x_2\)</span> refer to northing and easting coordinates. In general no more than 4 dimensions are used.</p><h4 id="random-function"><span class="header-section-number">1.2.2.2</span> Random Function</h4><p>The regionalized variables in general will possess both random and structured spatial characteristics. Let <span class="math">\(y(\mathbf{s_i})\)</span> denote the observed value of the variable of interest (ex: magnesium content) at location <span class="math">\(\mathbf{s_i}\)</span>. This can be regarded as a particular realization of the random variable <span class="math">\(y(\mathbf{s_i})\)</span> at the point <span class="math">\(\mathbf{s_i}\)</span>. The set of random variables: <span class="math">\(\{Y(\mathbf{s}) : \mathbf{s}\in Rcal\}\)</span> where <span class="math">\(Rcal\)</span> is the region of interest (ex: the agricultural field), is called a random function where</p><ul><li><span class="math">\(Y(\mathbf{s})\)</span> : the data response at location <span class="math">\(\mathbf{s} = (x_1; x_2)\)</span> or <span class="math">\(\mathbf{s} = (x_1; x_2; x_3)\)</span></li><li><span class="math">\(Rcal = \)</span> the set of all spatial locations in the study area.</li></ul><h2 id="classes-of-spatial-data"><span class="header-section-number">1.3</span> Classes of Spatial Data</h2><p>As outlined in Cressie’s book, spatial data generally fall into one of three categories:</p><ul><li>Spatially Continuous (Geostatistical or point-referenced) Data<ul><li><span class="math">\(Rcal\)</span> is a fixed subset of the plane of positive area (2-D) or volume (3-D).</li><li><span class="math">\(Y(\mathbf{s})\)</span> is a random variable at each of the <strong>infinite continuous</strong> locations <span class="math">\(s \in Rcal\)</span></li></ul></li><li>Area (Lattice) Data<ul><li><span class="math">\(Rcal = {\mathbf{s_1}; \mathbf{s_2};...;\mathbf{s_n}}\)</span> is a fixed regular or irregular lattice on the plane.</li><li><span class="math">\(Y(sidam)\)</span> is a random variable at each location <span class="math">\(sidam, i =  1;...;n\)</span></li></ul></li><li>Spatial Point Process Data<ul><li><span class="math">\(Rcal = {\mathbf{s_1}; \mathbf{s_2};...;\mathbf{s_n}}\)</span> is a random collection of points on the plane.</li><li><span class="math">\(Y(sdam)\)</span> is not specified or is a random variable at a location <span class="math">\(sdam\in Rcal\)</span> (marked point process).</li></ul></li></ul><h4 id="geostatistical-point-referenced-data"><span class="header-section-number">1.3.0.1</span> Geostatistical (Point-Referenced) Data</h4><p>The term “geostatistics” was coined by Matheron (1962, 1963) to describe the statistical methodology for examining ore reserves from spatially distributed data in an ore body.</p><p>/home/nguyenkinh/Dropbox/Hasselt/4sm/Disease Mapping/Selection_001.png</p><p>More generally, geostatistics refers to data from a random process <span class="math">\(\{Y(sdam): sdam\in Rcal\)</span> where <span class="math">\(Rcal\subset \mathfrak{R}^n \)</span> fixed, and <span class="math">\(sdam\)</span> is allowed to vary continuously over <span class="math">\(Rcal\)</span>. For example, if <span class="math">\(Rcal\subset \mathfrak{R}^n \)</span> is an agricultural field, and we are measuring the magnesium content in the soil(<em>Y</em>), then we can measure Y at any point <span class="math">\(sdam\)</span> within the field <span class="math">\(Rcal\)</span>.</p><ul><li>Important concepts: Stationarity, lsotropy and Variogram.</li></ul><h5 id="research-questions-and-methods"><span class="header-section-number">1.3.0.1.1</span> Research Questions and Methods</h5><p>Research Question</p><ul><li>interest focuses on modeling continuous spatial variation across space</li><li>spatial interpolation</li><li>estimating how spatial dependence varies with distance</li></ul><h5 id="methods-of-analysis-for-geostatistical-data"><span class="header-section-number">1.3.0.1.2</span> Methods of Analysis (for geostatistical data)</h5><ul><li><p>Linear Interpolation: Based in general on inverse-distance weighting, this method does not account for the variability in the data, measurements errors, distributional assumptions, etc. Although basic linear interpolation methods do not attempt to account for spatial correlation in the data, they can produce predictions of equivalent quality to methods which do account for spatial variability (such as kriging, described below).</p></li><li><p>Variograms: The variogram is a function of the distance and relative orientation of pairs of points which describes the degree of correlation between such points. Choice of a variogram model effectively assigns a spatial correlation structure to the data. This model can then be fit to the data and used to make predictions for the response variable at any point in the spatial domain R using kriging, as</p></li></ul><p>described below.</p><ul><li>Kriging: The method of kriging attempts to model the variability in the data as a function of distance, through the variogram and make predictions. “Kriging” is named after D.G. Krige, a South African mining engineer, who developed empirical methods for predicting the spatial distribution of ore grades based on a sample.</li></ul><p>1.3.2 Area (Lattice) Data</p><p>SMR (Urinary Bladder cancer among Males)</p><p>-d&quot; _‘ I! ,p</p><p>3m - 0-05 - 05-06 - 05-03 - QB- 12 - 12-15 - 1.5-2</p><ul><li><p>Under the assumption that we have data from the spatial process {Y(S) : s E lattice data refers to the case where R is some countable collection of spatial sites.</p></li><li><p>In other words, data can only be observed at the sites in R, and all subsequent</p></li></ul><p>inference applied only to those sites.</p><ul><li>Important concepts: Spatial Weights and Spatial Autocorrelation</li></ul><p>Research Questions and Methods</p><ul><li><p>Research Question</p></li><li><p>interest focuses on statistical inference</p></li><li><p>estimation, specification tests</p></li><li><p>hypothesis test on spatial randomness of attributes : value and location</p></li><li><p>Methods of Analysis:</p></li><li><p>Tests for Spatial Autocorrelation: In these discrete-space data, a common test performed is to assess whether there is any spatial autocorrelation present in the data. Some common tools for studying autocorrelation include the Geary’s C and Moran’s I statistics, and their corresponding randomization tests.</p></li><li><p>Median Polish: Iterative algorithm for removing large-scale row and column trend effects, before analysis on the small-scale spatial dependence is performed.</p></li><li><p>Nearest-Neighbor Models: These models make use of a spatial Markovian assumption to model the data (i.e.: The value of the random variable at a given site only depends on the values at a specified set of neighboring sites). These types of models lead to a number of so-called “auto”-models, (Autonormal, AutoPoisson, Autologistic, etc.) where the distribution of the random variable at a given site depends on itself through its dependence with the nearest neighbors.</p></li></ul><p>1.3.3 Spatial Point Processes R-</p><p>0.0 0.2 0.4 0.6 0.81.0</p><p>CELLS JAPANESE REDWOOD</p><p>1.0 0 w 0.0 .ﬁﬂ . 0.6 90 &gt;~ 0.4 0; 0.2 y . ‘ 0.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.3 1.0</p><ul><li><p>For this third type of spatial process {Y(S) : s E R is again considered to be a random index set for the locations of the process, but here, the data do not consist of realizations of some random variable at a given site. The data are the locations of the sites themselves, and the collection of all the sites is the event of interest. In this way, a measure such as the count of the number of items over any subset of R might be the key variable.</p></li><li><p>If a spatial point process {Y(s) : S E R} also consists of measurements {Z(S) : S E R} taken at the locations indicated by Y , the process is known as a marked point process, where the measurements in Z are known as the “marks”.</p></li></ul><p>f/éy</p><p>Research Questions and Methods</p><ul><li><p>Research Question</p></li><li>interest focuses on detecting absence of spatial randomness (cluster statistics)</li><li><p>clustered points vs dispersed points</p></li><li><p>Methods of Analysis:</p></li><li><p>Quadrat Methods: These methods involve counting the number of events (trees) in subsets of the region of study R, and comparing the observed frequencies to the expected frequencies under a Poisson process. The quadrats themselves are usually taken to be rectangular, but can be any shape desired.</p></li><li><p>Kernel Estimation: Letting represent the intensity parameter function for the Poisson process, kernel estimation can be used to estimate A.</p></li><li><p>Distance Methods: These methods take advantage of the precise distances between points, and often use nearest-neighbor methods to examine the distribution of sites (trees) in a given area. Such methods include the Ripley’s K-</p></li></ul><p>and L-functions, as well as the F- and G-functions.</p><p>Chapter 2 Exploring Areal Unit Data</p><p>m</p><p>2.1 Mapping Count Data</p><p>R</p><p>o A disease map provides instant visual information on variation of that disease throughout space.</p><p>o Naive use of mapping of health indicators can be misleading</p><p>o The choices of shadings, the scaling of the mapped index quantity, the number of risk classes and their delimitation have to be determined with care</p><p>o This choice depends on the range of variation, the precision of the estimates and the need for comparability over multiple maps.</p><p>0 When working with observational data, regions are not necessarily comparable</p><p>2.1.1 LIKAR data R; The Limburg cancer registry contains all records in Limburg from 1996-2005 which have been classified as cancers</p><p>http://likas.edm.uhasselt.be/</p><p>Disease Nurrber (Cervix cancer among Females)</p><p>32</p><p>Bladder Cancg</p><p>Disease Number (Urinaryr bladder cancer among Malena)</p><p>a “4%1 ‘gﬂeﬁy</p><p>£3.90&quot;</p><p>2.1.2 Issues with Crude Counts</p><p>0 Interpretation of crude counts is limited</p><ul><li>the population density should be accounted for</li><li>areas of high density of ‘at risk’ population would tend to yield high incidence</li></ul><p>of case-events 0 Crude counts of mortality/disease cannot be used to express an increased risk in certain regions.</p><p>o The incidence rate (IR, also called ‘crude rate’) expresses the number of new cases of disease occurring in a population of disease-free individuals in a specific period</p><p>of time _ _ - Total number of cases observed in study period</p><ul><li>Total number of people at risk</li></ul><p>Cervix Cancer: Crude Counts</p><p>Guide Rate (9’5) {Cervix cancer among Females)</p><p>J’l-i-II’ <em>..</em> .H“{ N” [ ,4: L-WEJ eff Tl, ._ “’5‘”J T1 in n mt“; _ “CT - LJ i,” If”, “L”? .l f _ __ a: ‘ if I L1 If .-~.r“”T ‘</p><p>i’ala - 0<em>01203 - U.W3-</em>0.EI:E - 0035-0119 - once-0.012 - 0012-0015 - war 0.015</p><p>R, 35</p><p>Bladder Cancer: Crude Counts</p><p>Crude Fiate {‘35) (Urinary bladder cancer aincng Males)</p><p>4i. _ m - 0*0.C|35 - ﬂmﬁrﬂﬁﬂﬂ - u.010–0.015 - QUE-0.1320 - HERD-0&amp;5 - war DIES</p><p>(R, 36</p><p>Example: age-specific disease rates</p><p>Are regions comparable?</p><p>0 Most diseases affect people of certain age disproportionally</p><p>o In general, there is an increasing incidence of cancer with age</p><p>Cervix Cancer Incidence</p><p>3 23-///<br />L g.</p><p>0 1134567090ﬂ2‘ﬁu‘lﬂﬂﬁ</p><p>mm</p><p>g i E</p><p>Urinary Bladder Cancer Incidence</p><p>3) an 0 0 128450700’DHE3145317I</p><p>Ago clue&quot;</p><p>R, 37</p><p>Example: age-distribution of population</p><p>Are regions comparable?</p><p>o The age-distribution in different regions is not the same</p><p>e We expect more cases for the region with more residents in the higher-age (and higher-risk) categories</p><p>Age Distribution Males Age Distribution Females as no “a / one 0.“ ~_ / on one //x V one am . , w /  ‘/ nos /’ . eon -‘X Y   ‘ . ea   no: i no. / \ .‘_ § m / / x. no: \’ ‘. / on / / on =.. « nor  x 001<br />one am izadso1eooiiizeueier7ie 12:4501aeciii2iaueioi1ie *0” Ancient MDT –H-ul -W -W HOT –ii-uI –b-n’.|d -Houapp-</p><p>2.1.3 Making Rates Comparable: Standardization R-</p><p>0 Incidence rates reflect estimated average risks for a study population</p><p>0 Thus, populations containing more people in higher age ranges will have higher summary incidence rates than those of younger populations.</p><p>0 As a result, the incidence rates for two regions may appear different, but this difference may be due to the different age distributions within the regions rather than to a difference in the underlying age-specific risk of disease.</p><p>0 Conclusion: the spatial variation of background population should also be accounted for</p><p>Rate standardization is a mechanism to adjust summary rates to remove the effect of known risk factors (such as age) and make rates from different populations comparable.</p><p>Rate Standardisation</p><p>The idea of rate standardisation is to</p><p>0 select a standard population, and</p><p>o adjust observed rates to reflect the age distribution within the standard population.</p><p>Possible standard populations:</p><p>e A superpopulation containing the study population</p><ul><li>e.g. the Belgian, European or World population</li></ul><p>e The total subpopulation</p><ul><li>if we are interested in standardising some subset of individuals, e.g., from a particular region within the study area</li></ul><p>Notation</p><p>0 Assume we have G age groups 9 (g =1,…,G) o ygiz number of cases in age group 9 for study population i</p><p>0 ng,: number of people at risk in age group g for study population i</p><p>0 r9,- = is the observed incidence proportion in age group g for the study population i</p><p>’ yr = :9 ygi</p><p>o n,- = :9 n9,</p><p>0 yg, n5, r5, yS, n3 denote the same quantities for the standard population</p><p>(m</p><p>Direct Standardisation</p><p>o How many cases would we observe in the standard population the observed age-speciﬁc rates of disease applied? {Mausner and Kamer, 1985)</p><p>e The expected number of cases in age group 9 for the standard population is</p><p>S __ _ S _ ygi S Egi - Tglng ‘ frag</p><p>e The overall erpected rate for the standard population is</p><p>g s _ :9 E5; = 9 “any n5 :9 In; 29 n3</p><p>This is the directly standardised rate.</p><p>o This is a weighted average of the observed age-specific rates in the study population, with weights corresponding to the numbers at risk in each group</p><p>within the standard population</p><p>0 Comparative mortality ﬁgure (CMF) is defined as</p><p>E5 ES- 0 M F, 2 is Z M y 2ng o This is related to the directly standardised rate: E E5?- 2993 —S : CMF,- &gt;l&lt; —S Zg n9 :9 n9 Thus, the directly standardised rate is equal to the CMF multiplied by the crude incidence proportion in the standard population.</p><p>43 (R?</p><p>0 Direct standardisation requires the following data (lnskip 1988)</p><p>gg_ .2: mm ed, bserv ’ n o latua opu he study p t ﬁar &quot; rates ihc -spec /D</p><ul><li><p>Number of people at risk in the standard population, n3</p></li><li><p>Total number of cases observed in the standard population, :9 y;</p></li></ul><p>0 Issues:</p><ul><li><p>Direct standardisation requires accurate assessment of the age-specific incidence proportions for the observed study population, r9,- : ygi/ngi</p></li><li><p>For rare diseases, r9,- may be statistically unstable, especially when n9,- is small</p></li><li><p>If n9,- is small, addition of a single case could drastically change the value of r9,-</p></li><li><p>Also, age-specific incidence counts yg, may not be as readily available as the total observed incidence counts and the age-specific population counts within the study population observed, due to confidentiality issues</p></li></ul><p>Indirect Standardisation</p><p>o What would be the number of cases expected in the study population people in the study population contracted the disease at the same rate as people in the standard population? (Mausner and Kramer, 1985)</p><p>e The expected number of cases in age group 9 for the study population is</p><p>yS E __ S .__9 . n9</p><p>e The overall erpected number of cases for the study population is</p><p>Z 2 : S _ 3-9 . ., ,. n 9</p><p>e The standardised mortality ratio (SMR) is defined as</p><p>3h :9 ygi : - = ’- EZ’ :9 Egi</p><p>c When referring to incidence rather than mortality, this is called standardised morbidity ratio (SMR) or standardised incidence ratio (SIR)</p><p>o SMR &gt; 1.0 indicate more cases observed in the study population than expected based on the age-specific incidence proportions from the standard population</p><p>0 These SMRs are often the basis for atlases of disease risk (Pickle et al. 1999)</p><p>e The indirectly standardized rated is the product of the SMR and the crude rate in the standard population: 5 Siva/29% Zg’ng (cfr. CMR) 0 Indirect standardisation requires the following data (lnskip 1988) s - Age-specific rates for the standard population, r3 = i-gs 9</p><ul><li>Number of people at risk in the study population, n9,-</li><li>Total number of cases observed in the study population, y, = 29319,-</li></ul><p>47 (R?</p><p>e Often, aggregate (marginal) standards are used in indirect standardisation. The SMR for region i is: » Z w E w SMRZ.:%:79J:’LL (21) $ng Zg&lt;-%:Z::&gt;mg</p><p>This is called internal standardisation.</p><p>0 Internal standardisation is in some sense ‘cheating’, since we are ‘losing a degree of freedom’ by estimating the rate from our current data Z Z9 0 Alternatively, one can refer to an existing standard table of age-adjusted rates for the disease (which are available for many types of cancer). This is called external</p><p>standardisation</p><p>Cervix Cancer: indirect standardisation</p><p>SMR (Cervix cancer among Females)</p><p>..__‘I ’i r”-’“–<em> .’ 1 jog. “WI 5.” i&quot; I I</em> I. ____.:. l-‘j: .I -‘l. l’ ’</p><p>smr - 0-0.5 - 0.5-0.6 - 0.6-0.0 _ 0.8-1.2 -1.2—1.5 -1.5-2 -2–15 -22.‘IB</p><p>49</p><p>Bladder Cancer: indirect standardisation</p><p>SMR (Urinary Bladder cancer among Males)</p><p>ir’x“ _–<strong> r -&quot; 15“: I il all” “‘’- ‘i_ ’i a: .- ___ i J- .I “a ’ -. ’- T I’I’L-l. ir.-”“-’ - I -. i” I l .-’ I.- .r. .- _ . .. -. 1. .j I, J: i T _ h”. ;. .-&quot; l~ ’ g</strong> “L‘J- I __I _-..__, .</p><p>:n’r _ III-0.5 - 0.5-0.5 - 0.5-0.3 - 0.3-1.2 - 12-15I - 1.5-2</p><p>R, 50</p><p>2.1.4 Summary</p><p>0 Direct Age Standardisation</p><ul><li>Adjust observed rates to reflect rates that we would observe if the observed age-specific rates applied to the standard population</li><li>Apply the observed age-specific rates directly to the standard population</li></ul><p>0 Indirect Age Standardisation</p><ul><li><p>Adjust observed rates to reflect rates that we would observe if the population standard’s age-specific rates applied to the study population</p></li><li><p>Apply the age-specific rates from the standard population to estimate indirectly the numbers of cases expected in each age group in the observed</p></li></ul><p>study population</p><p>e The choice between direct and indirect standardisation often reduces to the type of data available</p><p>e Often, the standard population is much larger than the study population, and the values r39 = may provide more stable estimates than rj = yj/nj</p><p>e For indirect standardisation, an internal standard population (superpopulation containing th regions of interest) is preferred as compared to an external standard population (entirely seperate population), for comparability of the regions</p><p>(Breslow and Day 1975).</p><p>e In addition to age standardisation, we could also standardise rates to compensate for other risk strata (e.g. gender, race, etc).</p><p>/</p><p>Always be cautious with interpreting and comparing maps!</p><p>|//</p><p>R, 53</p><p>2.2 Making Chloropleth maps</p><p>e To visualize attribute data of spatial tessellations, a map called “Choropleth map” is used. 0 Choropleth map is a map showing attribute data of a spatial tessellation by colors and textures. - Categorical variable is directly visualized by colors and textures: different colors indicate different categories.</p><ul><li>Numerical variable is first categorized into several classes, and then visualized by a progression of colors and textures.</li></ul><p>e To make a Choropleth map of a numerical variable, we determine 1. classification scheme 2. class number 3. class boundaries</p><ol start="4" style="list-style-type: decimal"><li>colors and textures.</li></ol><p>R 55</p><p>2.2.1 Classification of Schemes</p><p>R-</p><p>0 There are four schemes for categorizing numerical variables. 1. Equal interval scheme 2. Quantile scheme 3. Nonuniform scheme</p><ol start="4" style="list-style-type: decimal"><li>Irregular interval scheme</li></ol><p>R, 56</p><p>Equal interval scheme</p><p>Equal interval scheme categorizes a numerical variable by an equal interval value. If we specify interval value, GIS calculates boundary values, classifies spatial units into categories, and visualizes the categories by different colors or textures.</p><p>84 149 74 162 91 I ‘.’ ii. a: = EEEE = iﬂ: . ea: .. H : :EEH - . .:-.-= ..: ea; &lt; I ii: iii: I EEEE Ea: ea: res. ..::..=. ma: . EH:- EEEEH ea: IIIIIIII III II I IIIIII IIIII II IIIIII =IIIIIII :IIIII I EIIIII IIEIIIII IIIIII I ease: em: .=:==:: =:.=:::: IEEEEEIE 0 20 40 60 80 100</p><p>R 57</p><p>W</p><p>Quantile scheme categorizes a numerical variable so that every category has the same number of spatial units. In this scheme we give the number of categories instead of interval value. GIS then calculates boundary values, classifies spatial units into categories, and visualizes the categories by different colors or textures.</p><p>100 100 100 100 100 g E II a. . a: = III I :- i :3! ii a! . i a: i. . .a: .:== .H I a: a: i ii: iii: . ‘EIE l E: ii: .i: ea: ea. :E:=:-.= a: 3:: iii. .aiaiii #53:: aiaiai’ai Ei’ ========= sea-3’: dam: . =i====!== a! .=====:=== iaiaiai: =’=====.= 0 22 32 58 78 100</p><p>Nonequal interval scheme</p><p>When we are interested in a certain range of attribute value, we want to use finer categories in the range. In this case we use nonequal interval scheme. Typical examples include monotonically increasing (decreasing) interval scheme, in which the interval monotonically increases (decreases) with attribute value.</p><p>0 33.3 60.0 80.0 93.3100 Ambulch # |__-l I—|I-.-I|-l progressron 33.3 26.0 20.0 13.3 6.7 03.2 9.7 22.6 48.4 100 3.2 6.5 12.9 25.8 51.6 , Geometric f-fm progressm&quot; 0 51.6 77.4 90.393190 I<em>–II–AL<strong>II<em>ILI 51.6 25.8 12.9 6.53.2 0 10.7 26.7 47.5 72.5 100 l<strong>ll</strong></em>||-</strong>ll</em>.#l_-A 10.7 16.0 20.8 25.0 27.5 Il‘l‘egulal’ly increasing I I I I I I progression 0 9.3 20.4 34.9 56.6 100 HL-JI-Ah-II’A 9.3 11.1 14.5 21.7 43.4</p><p>R, 59</p><p>Irregular interval scheme</p><p>Frequency distribution of attribute value often shows “breakpoints”, where the frequency suddenly drops. In such a case, we can obtain a natural classification of attribute variable if we take the breakpoints as boundaries of intervals.</p><p>Breakpoints I = I E: III I III I :a: a iiiii E: . ea: a: H II=III I II III i. -=.::: i: i a: E’:’ =:.: E:-:-: ii. i .=:: iii! I IIII III-III III III I IIIII IIIII :.====:.::i=:=: ii: EEE. =.=:=.-.:= .iiei Egggggggggggﬁg EEEEEEE EEEEEE: aim: . ideas: idem: made 0 34 56 78 100</p><p>(R, 60</p><p>2.2.2 Class Number and Colors R</p><ul><li><p>In theory, we can use any number of categories in classification of numerical variables. You may think that the more categories you use, the better map you obtain. In practice, however, we can discriminate only a limited number of colors used for visualizing categories. It is not always useful to increase class number.</p></li><li><p>Class boundary greatly affects the appearance of Choropleth maps that represent numerical variables. We should be careful when classification scheme involves subjective choice of class boundaries, as seen in nonequal and irregular interval schemes.</p></li><li><p>Black/white, colors, red-to-green, rainbow, light-to-dark colors,</p></li></ul><p>Effect of the size of spatial urE</p><p>In Choropleth map, map readers tend to pay attention to large polygons while they often overlook small polygons. Attribute data of large polygons are more influential than those of small polygons in the perception of map readers. Large polygons are emphasized as a result of visualization. This often leads to misunderstanding of the</p><p>spatial distribution of attribute values.</p><p>Example: effect of categorization</p><p>0 Equal Intervals: for fairly uniformly distributed data 0 Quantile map: 4 classes, lowest quartile, second quartile, third and highest</p><p>0 Standard Deviates: divide data into units of standard deviation around the mean</p><p>o Log-Scale: interpretation on log-scale</p><p>Equal Interval Quantlles ‘ 0 Equal Intervals SR-Alﬂlmll Ouanules I 0 11:11.44 (4) SR-Amime I 11441603806) I o muse (5) I 0.8816132 (14) I 04561110941 (13) I 132161.16 (5) I 11.941161493 (14) I 1.151621 (1) I 1.493m222 (3) 1] another: (4) [I arm&quot; (4)</p><p>I</p><p>I</p><p>I</p><p>Standard Devlation 0 standard Deviation SR-Asﬁirm I 0 (00.086 (2) I 0086m0548 (e) I 0545161009 (17) I 11109111141 (12) I 1.41 161.931 (3) El nether: (4)</p><p>Log-scale l Standardized Prevalence Rates Asthma</p><p>I o to 05 (6) I 05111 as (i) I 0616 03 (6) I 03111 12 (14) I 1.211: 1.5 (9) I 1.51:: 2 (2) I 2 (1115 (2) El ulnthers (4)</p><p>Alternative: independent categorization</p><p>e Bi-chromatic range from red to green 0 Based on a uniform log-scale division (Knorr-Held and Raser, 2000)</p><p>e 7 categories with a flexion zone in yellow centered around the median</p><p>0 to 0.5 0-5 to III-E- ﬂ-Eto Ill-B D-Bto 1-2 1200 1-5 1-5 to 2</p><p>2 to 15</p><p>0 Dark red regions indicate a high risk of disease</p><p>0 Dark green indicate a small risk</p><p>R, 65</p><p>2.2.3 Code in R R-</p><p>library(maptools)</p><p>becounty.shp &lt;- readShapePoly(”comgem.shp“) summary(becounty.shp) databe1&lt;-as.data.frame(becounty.shp©data) fix(databe1)</p><h1 id="map-of-the-different-regions-in-belgium"><span class="header-section-number">2</span> Map of the different Regions in Belgium</h1><p>plotvar &lt;- becounty.shp©data$LOC</p><p>plotclr &lt;- c(”red”,”green”,”b1ue”)</p><p>class &lt;- as.numeric(plotvar)</p><p>colcode &lt;- p10tclr[class]</p><p>p10t(becounty shp, Xlim=c(22265, 295157), ylim=c(21163,244028)) p10t(becounty shp, col=colcode, add=T)</p><p>tit1e(1ist(”Be1gian Regions”, cex=4.5, col=”b1ack“))</p><p>(R, 66</p><p>library(RColorBrewer) library(classlnt)</p><h1 id="equal-frequency-class-intervals-map"><span class="header-section-number">3</span> equal-frequency class intervals Map</h1><p>nclr &lt;- 4</p><p>plotclr &lt;- brewer.pal(nclr,“Reds”)</p><p>class &lt;- classlntervals<plotvar, nclr, style=”quantile")
colcode <- findColours(class, plotclr)

p16t(becounty.shp, Xlim=c(22265, 29157), ylim=c(21163,244028))
plot(becounty.shp, col=colcode, add=T)
title(main=”Cancer incidence in 2006”,
sub="Quantile (Equal-Frequency) Class Intervals")
1egend(22267, 100300, legend=names(attr(colcode, "table”)),
fill=attr(colcode, "palette”), cex=1, bty="n")


- Visualizing data in the attribute space and geographical space simultaneously

- Useful for exploring spatial stationary (homogeneity) of spatial patterns and

pI’OCCSSGS

a if, -
an)
. a wr’nlit
a: 4 r, , 39514121 (3
M 1411.41.54»: ‘“ " v :5.

. ﬁrst)” 1%  we: :31 o.

l? by?“ *ﬁﬁt’viie’lﬁrh‘lp 1" “‘ 15w)
I»§¢“£¢nﬁ'ﬁr’.ﬁﬂaﬁ’:;$4"
ma) 41.44114131441111415
«He’de ~21“) “214421.; 1‘ ' .

41441111114444.4111
awareness
3115831153531”

[1 unde1141  1057",? .

- 141-146 1' 053m“

1] 146-153 “gig

I: over153 my“

41.4
"a‘r.

(R,
68

Several maps can be produced depending on the style used when we define the

" classlntervals".
- The "fixed" style permits a "classlnterval" object to be specified with given breaks
- The "equal" style divides the range of the variable into n parts.
- The "quantile" style provides quantile breaks
- The "sd" style chooses breaks based on pretty of the centred and scaled variables
- The "hclust" style uses hclust to generate the breaks using hierarchical clustering

- The “bclust” style uses bclust to generate the breaks using bagged clustering;


2.3 Defining the Neighbourhood Structure

- Defining the relationship between regions

- Spatial weights define the spatial relationships among spatial objects (e.g.,
polygons, rasters, points)

- Spatial weights are used to identify spatial contiguity or neighbourhood of a given

object

- Spatial matrix ( for n objects, there will be n X n pairs of relationships)


2.3.1 Spatial Weights
Ry
Depending the case we choose (Rook’s or Queen’s case) the weight matrix can be
constructed and give a weight equals 1 to those cells in the matrix reflecting
contiguity in the space.

Rook's Case Queen’s (King) Case

Ill. IgI
In. In.
III III

Adjacent Cells Adjacent and
Diagonal Cells


Example

Eli Eli

“III III!

Bull“ nun

III! I!

man I!"

III “III!
’/’
72

Remark
Other weights matrices can be used as well.

- The distance between spatial locations, can be used to construct the weights, in

general the weights are
1

Egg
, and 07 is taken to be 1.

- Other approaches consider a threshold distance ht, and if the distance is larger
than ht, then the weight is taken equal to 0.

In previous example, the weight matrix is constructed based on the distances between

the centroids of each spatial region.


Example

an
u
an
u
an
II
III:
I.
an
I.
an
n

(R?
74

2.3.2 Code in R
R-

# constructing the weight matrix for rectangular grid
######################################################

N <- 3; X <- matriX(1:(N‘2),nrow=N,ncol=N)

rowdiff <- function(y,z,mat)
{abs(row(mat)[y]-row(mat)[z])}

coldiff <- function(y,z,mat)
{abs(col(mat)[y]-col(mat)[z])}

rook.case <- function(y,z,mat)
{coldiff(y,z,mat)+rowdiff(y,z,mat)==1}
bishop.case <- function(y,z,mat)
{coldiff(y,z,mat)==1 & rowdiff(y,z,mat)==1}
queen.case <- function(y,z,mat)
{rook.case(y,z,mat) I bishop.case(y,z,mat)}


matriX(as.numeric(sapply(x,function(y)sapply(x,rook.case,y,mat=x)))
,ncol=N“2,nrow=N“2)
matriX(as.numeric(sapply(x,function(y)sapply(x,bishop.case,y,mat=x)))
,ncol=N“2,nrow=N“2)
matriX(as.numeric(sapply(x,function(y)sapply(X,queen.case,y,mat=x)))
,ncol=N“2,nrow=N“2)

### Using distance to create the weights from centroids
##########################################################
library(maps)

library(maptools)

library(spdep)

belshape <-readShapePoly("limburg.shp");summary(belshape)
coord<-coordinates(belshape)
I- <- belshape©data$NlS


### Define neighborhood based on k nearest neighbors
belkl <- knearneigh(coord,k=2)

#converts a knn object returned by knearneigh into a neighbours list
belknnl <- knn2nb(belk1,sym=TRUE); summary(belknn1)

#neighouring municipalities
belk1$nn

#plot of neighborhood structure
plot(belshape,border="grey”)
plot(belknn1,coord,add=TRUE,col="red”)

#number of neighbors
card(belknn1)

#give distances between neighbours
beldistl <- nbdists(belknn1,cbind(belstations$x,belstations$y))


beldistvec <- unlistheldistl)
belmaXd <- maX(be1distvec)

### Define neighborhood based on distances
#############################################
beldnb <- dnearneigthoord,O,(belmaxd+0.01) ,row.names=ID)

summary(beldnb)

#plot of neighborhood structure
plot(belshape,border="grey")
plot(beldnb,coord,add=TRUE,col=”red”)

#number of neighbors
card(be1dnb)

#matriX format
nb2mat(beldnb)

m

#list of neighbors
lapply(1:length(beldnb), function(i) beldnb[[i]])

### Define neighborhood based on sharing boundaries
###################################################
belqnb<-poly2nb(belshape,queen=T)

summary(be1qnb)

#plot of neighborhood structure
plot(belshape,border="grey")
plot(belqnb,coord,add=TRUE,col="red")

#number of neighbors
card(belqnb)

#list of neighbors


lapply(1:length(belqnb), function(i) belqnb[[i]])

(R,
80

Remark

In “spdep” package, this is implemented in the “dnearneigh function, which takes as
input a matrix of coordinates and a lower and upper distance bound (as well as,
optionally, a variable containing region ids). All points that are within the defined
distance band from each other are categorized as neighbors. It is important to note
that the distance calculation applies only to Euclidean distance, which requires that
the coordinates are projected (not simply latitude, longitude). A slight complication is
that the value for the lower and upper distance bound must be specified beforehand.
Typically the lower bound is zero, but the upper distance bound should be such that

no observations become islands.


2.4 Spatial Autocorrelation

- Measuring Dependency

- The objective is to:

- measure how strong the tendency is for observations from nearby regions to be
more (or less) alike than observations from regions farther apart,

- judge whether any apparent tendency is sufficiently strong that it is unlikely to
be due to chance alone.


2.4.1 Definition Autocorrelation

R-

- Complicated name, simple concept...
- Expresses the amount of spatial dependence
- How much proximity matters in spatial data
- Correlation is the key notion
- It indicates how much two properties vary together
- Correlation of a variable with itself through space
- Is a variable in a location correlated with its values in nearby places?

- Spatial + auto + correlation

(R,
83

- If there is any systematic pattern in the spatial distribution of a variable, it is said
to be spatially autocorrelated

- If nearby or neighboring areas are more alike, this is positive spatial autocorrelation
- Negative autocorrelation describes patterns in which neighboring areas are unlike.
- Random patterns exhibit no spatial autocorrelation

- It measures the extent to which the occurrence of an event in an areal unit
constrains, or makes more probable, the occurrence of an event in a neighboring
areal unit.


W

- Most statistics are based on the assumption that the values of observations in
each sample are independent of one another

- Positive spatial autocorrelation may violate this, if the samples were taken from
nearby areas

- Assumption that observations have been selected randomly, not valid.
- Estimates obtained will be biased and overly precise.

- Biased because areas with higher concentration of events will have a greater
impact on the model estimate

- Overestimate precision because, since events tend to be concentrated, there are
actually fewer number of independent observations than are being assumed.

- Goals of spatial autocorrelation

- Measure the strength of spatial autocorrelation in a map

- Test the assumption of independence or randomness


2.4.2 Global Indicators of Spatial Autocorrelation
R

- Let Y,- denote the response at the i-th location  = 1;...;n).

- Let SM be a measure of how similar or dissimilar the responses are at locations i
and j.

- Let WU be a measure of the spatial proximity of locations i and j.
- For future reference, define matrices S : (Sij) and W = 
- Most global indexes of spatial autocorrelation are of the form

ZZwa

i=1 j=1



i=1 j=1

/

- This is also called general cross-product statistics (Mantel, 1967)


2.4.3 Moran’s I

- One of the oldest indicators of spatial autocorrelation (Moran, 1950). Still a
defacto standard for determining spatial autocorrelation.

- Applied to zones or points with continuous variables associated with them.

- Compares the value of the variable at any one location with the value at all other

HZZWvll/FY) (VJ-Y)
I = 

locations



- Note that I resembles ordinary correlation coefficient (but is not restricted to
I1 I < 1)


Properties Moran’s I

. . . 1
- Expected value of I under assumption of independence (E(I)) is -n-_-1

- If I > E(I) :&gt; positive autocorrelation (clustered pattern) - If I &lt; E(I) :&gt; negative autocorrelation (regular pattern)</p><ul><li><p>Testing for Spatial Autocorrelation, can be done using randomisation or Normal approximation</p></li><li><p>Randomization distribution obtained by reassign data values among the n fixed locations. If I lies in the tails of this distribute, reject assumption of independence.</p></li><li><p>Normal approximation under independence (n &gt; 25):</p></li></ul><p>I - E(I) lel : «Var(I) 2 2 MEWin + 3 W17) - We) Var(I) =</p><p><n2 - 1> 12%-</p><p> I] / Compare observed Z-score Z = (I - /1/Var(I) to a standard normal</p><p>distribution</p><p>WE</p><ul><li>Spatial similarity is assessed by deviations of each regional count with overall mean</li><li>Does this assess clustering?</li><li><p>Regional at-risk population sizes may result in variations</p></li><li><p>Replace counts with incidence proportions, to remove impact of population heterogeneity</p></li><li><p>This removes heterogeneity in the expected counts, but local incidence proportions remain heterogeneous because of differences in sample size</p></li><li><p>An alternative approach using constant risk hypothesis (Walter, 1992): Y,- - rn, - rn, 2 2 WI- -: f; , j rn, rn, [0,. : (z/ (22%) i j</p></li></ul><p>Icr</p><p>2.4.4 Geary’s c</p><ul><li><p>A similar statistic is proposed by Geary (Geary, 1954)</p></li><li><p>Interaction is not the cross-product of the deviations from the mean, but the deviations in intensities of each observation location with one another</p></li></ul><p>(n - 1) Wijlyi - YjI2 2 _ Y&gt;2</p><p>C:</p><ul><li><p>If regions i and j have similar values, (Y; - will be small</p></li><li><p>Scaled by overall variation around mean regional observation Y</p></li><li><p>Also called, Geary’s contiguity ratio</p></li></ul><p>Properties of Geary’s c</p><ul><li>Value ranges between 0 and 2</li><li>0 indicates perfect positive spatial correlation</li><li><p>2 indicates perfect negative spatial autocorrelation</p></li><li><p>If value of any one zone are spatially unrelated to any other zone, the expected value of c will be 1</p></li><li><p>Does not provide identical inference because it emphasizes the differences in values between pairs of observations, rather than the covariation between the pairs.</p></li><li><p>Geary’s c corresponds to the Durbin-Watson statistic (used to test for serial autocorrelation)</p></li><li><p>Testing can be done in a similar way as before: using normality assumption or via randomisation</p></li><li><p>Moran’s I gives a more global indicator, whereas the Geary’s coefficient c is more</p></li></ul><p>sensitive to differences in small neighborhoods.</p><ul><li>Geary’s c can be adjusted in similar way as I6, by replacing Y, by standardised</li></ul><p>value</p><p>(R, 93</p><p>2.4.5 Code in R R-</p><p>library(spdep) # Moran’s I moran(x=Y,listw=col.W,n=length(beldist1), SO=Szero(col.W))</p><p>nsim &lt;- 999; set.seed(1234) moran.mc(x=Y, 1istw=col.w, nsim)</p><p>moran.test(x=Y, listw=col.W) # Geary’s C</p><p>geary(x=Y,listw=col.W,n=length(be1dist1),n1=length(be1dist1)-1, SO=Szero(col.W))</p><p>(R? 94</p><p>nsim &lt;- 999; set.seed(1234) geary.mc(x=Y, listw=col.w, nsim)</p><p>geary.test(x=Y, listw=col.W)</p><p>R, 95</p><p>2.4.6 Local Indicators of Spatial Autocorrelation (LISA)</p><ul><li><p>Global indicators of spatial association assess patterns of spatial similarity summarised over the entire study area</p></li><li><p>Detect clustering</p></li><li><p>What if we want to find out local pockets of mutually similar deviations from the</p></li></ul><p>overall mean? - Use a local indicator</p><ul><li><p>Compares local value to that of its neighbours</p></li><li><p>Detect individual clusters</p></li></ul><p>(R, 96</p><p>Local Moran’s I index - Basic form of LISA for region i is: 2 W434 with Wij row-standardised weights (rows sum to 1)</p><ul><li>Local Moran’s I index can be calculated as:</li></ul><p>(YE - Y) Z Wall/j ‘ l7)</p><p>/</p><p>[1:71</p><p>Zoo- - 7r</p><p>i</p><p>R, 97</p><p>Moran Scatter Plot</p><ul><li><p>A Moran Scatter Plot can be usefull to detect local pattern of spatial association</p></li><li><p>if the plot is divided in quadrants, then</p></li><li><p>In the first quadrant are the locations in which the value is high and the neighbors are also having high values (positive autocorrelation)</p></li><li><p>The second contain locations with low values surrounded by locations with high values (negative autocorrelation)</p></li><li><p>The third contain locations with low values and the neighbors have also low values (positive autocorrelation)</p></li><li><p>The last one contain those locations with high values surrounded by locations</p></li></ul><p>with low values.</p><p>2.4.7 Code in R</p><p>R-</p><h1 id="lisa"><span class="header-section-number">4</span> LISA</h1><p id="section"><span class="header-section-number">4.0.0.0.0.0.1</span> </p><p>localmoran(x=Y,listw=col.W,p.adjust.method=”bonferroni“)</p><p>moran.p10t(x=Y,listw=col.W,p.adjust.method=”bonferroni”, labels=be1shape©data$municipali)</p><p>localG(x=Y, listw=col.W)</p><p>(R, 99</p><p>Example</p><p>i ‘e O 9 I . 0 O ‘2 I ‘- I ll 0 . I | O .. : ° : (- I 9’ i N I ‘- i i i . i &gt;‘ I (U ______________________-<strong>-</strong>-__ <strong>-</strong>_________-____ e N u (0 ‘- I 17&quot; ol 0 ° 7’ I - i ‘O i a) V‘- : C) N I U) - I m i &gt; I _ i a 1’ ° .43 i a - : in (‘1‘ : l I O 239 iI . I I 0 : N . ‘- I I i l i 290 1 l (D. 309 Il ‘<em> ‘</em></p><p>b 100</p><p>belstatSTday</p><p>W</p><ul><li>Identify Hot Spots</li><li>significant local clusters in the absence of global autocorrelation</li><li>some complications in the presence of global autocorrelation (extra</li></ul><p>heterogeneity)</p><ul><li>significant local outliers</li><li><p>high surrounded by low and vice versa</p></li><li><p>Indicate Local Instability</p></li><li><p>local deviations from global pattern of spatial autocorrelation</p></li></ul><p>Chapter 3 Modeling Areal Unit Data</p><p>3.1 Aggregate Count Data</p><p>R</p><ul><li>In spatial epidemiology, focus is often on tract count data</li><li><p>Common to use the standardised mortality (or morbidity) ratio (SMR)</p></li><li>Notation:</li><li><p>Y, : observed number of cases of disease in county i</p></li><li><p>E,- : expected number of cases of disease in country i</p></li><li><p>Y, are random, but the E, are thought of as fixed and known.</p></li></ul><p>m</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>values for SMR<br />values for SMR I (5) &lt; 0.5 4 0.5 - 0.0 , Io I (a) 0.0- 0.3 ﬂ [1 (14) 0.8 - 1.2 ’ I (5) 1.2 - 1.5 I (5) 1.5 - 2.0 - I (31&gt;: 2.0 2.00E+4km «if-H</p><p>3.1.1 Traditional Models and Methods</p><p>M-</p><ul><li><p>The usual model for the Yi- is the Poisson model: Y1 N P0i880n(EZ-6i) where 91- is the relative risk: of disease in region 2’</p></li><li><p>This corresponds to the log-Likelihood:</p></li></ul><p>6 : i - Em: i=1 i=1</p><ul><li><p>Maximum likelihood estimator of 6%- is the SMR</p></li><li>The estimate variance is Var(SMRi) = mum/E? : ej/EZ- : yg/Ef</li><li><p>Thus, the estimated standard error is 52- =</p></li><li><p>Wald-based confidence intervals for the SMRs can then be calculated [SMR -1.968¢,SMR + 1.9681}</p></li><li><p>This assumes that the SMRs are normally distributed</p></li><li><p>This is a bit awkward since the data are discrete and SMR can be positive only</p></li><li><p>Alternatively, assume log(SMRi) is roughly normally distributed</p></li><li><p>Using the delta method, one can find that</p></li></ul><p>1 E3 Y,- 1</p><ul><li><p>An approximate 95% CI for log&lt;SMRi) is thus [iog(SMR,-) - 1.96/ iog(SMR,-) +1.96//?,-]</p></li><li><p>Back-transforming gives an approximate 95% CI for SMRZ-z</p></li></ul><p>[SMRiexp(-1.96//E), SMRiexp(1.96//}7,)] - This is equal to the Cl [SMRi/errfac, SMR,- &gt;l&lt; errfac] with error factor (Clayton and Hills, 1995) e fc z 1 rr a :ex _0, H- P 1 /2 Yi</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>// Community Y E SIR ci95%(SIR) Community Y E SIR ci95%(SIR) lowlim uplim lowlim uplim ALKEN 7 5 1.41 0.67 2.96 KINROOl 3 5.1 0.59 0.19 1.82 A5 5 3.2 1.54 0.64 3.7 KORTES 4 3.6 1.11 0.42 2.95 BERlNGEN 14 17.3 0.81 0.48 1.37 LANAKEN 8 11.1 0.72 0.36 1.44 BILZEN 7 13.4 0.52 0.25 1.09 LEOPOL- 10 6.4 1.56 0.84 2.9 BOCHOLT 6 5.2 1.15 0.52 2.56 LOMMEL 9 13.6 0.66 0.34 1.27 BORGLOON 7 5.1 138 0.66 2.89 LUMMEN 6 6.3 0.96 0.43 2.13 BREE 1 6.5 0.15 0.02 1.09 MAASEIK 12 10.5 1.14 0.65 2.01 DIEPENBEEK 7 7.7 0.91 0.43 1.91 MAASM 18 15.8 1.14 0.72 1.81 DlLSEN 18 8.2 2.2 1.39 3.49 MEEUWEN 8 5.2 1.52 0.76 3.05 GENK 28 27.6 1.01 0.7 1.47 NEERPELT 6 6.9 0.87 0.39 1.93 GINGELOM 7 3.8 1.83 0.87 3.83 NIEUWERK 2 3.1 0.65 0.16 2.6 HALEN 1 4 0.25 0.03 1.76 OPGLAB 2 3.9 0.52 0.13 2.07 HAM 6 4.2 1.41 0.64 3.15 OVERPELT 5 5.7 0.88 0.37 2.12 HAMONT 3 6.2 0.49 0.16 1.5 PEER 7 6.4 1.09 0.52 2.28 HASSELT 35 33.9 1.03 0.74 1.44 RIEMST 2 7.4 0.27 0.07 1.08 HECHTEL 4 4.9 0.82 0.31 2.19 SINTVTRUI 38 18.8 2.02 1.47 2.77 HEERS 3 3.3 0.91 0.29 2.81 TESSEN 4 7.2 0.55 0.21 1.47 HERK 7 5.4 1.3 0.62 2.73 TONGER 19 14.9 1.28 0.81 2 HERSTAPPE 1 0 22.18 3.12 157.48 VOEREN 0 2 O HEUSDEN 8 12.9 0.62 0.31 1.24 WELLEN 2 3.1 0.64 0.16 2.57 HOESELT 7 4.2 1.67 0.8 3.5 ZONHOV 6 8.6 0.7 0.31 1.56 HOUTH 8 12.3 0.65 0.33 1.31 ZUTEN- 2 3 0.68 0.17 2.7</p><p>m</p><p>M</p><p>Suppose we wish to test whether the true relative risk in region 7L is elevated or not</p><p>H02921versusHA291&gt;1</p><p>0 Under the null hypothesis, Y,- N Poisson(Ez’)</p><p>0 Thus the (one-sided) p-value for this test is</p><p>p = P(X 2 WE) = 1- P(X &lt;</p><p>Y–1</p><p>Z exp(-E,-)Ef</p><p>= 1 - Z T z=0</p><p>o It 19 &lt; 0.05 we would typically reject H0 and conclude that there is statistically significant excess risk in region 1.</p><p>Remarks</p><ul><li>Drawbacks of the traditional Poisson model:</li></ul><p>0 can yield large changes in estimate with relatively small changes in expected value (since they are based on ratio estimators)</p><p>0 when a (close to) zero expectation is found, the SMR will be very large for any positive count</p><p>o the zero SMRs do not distinguish variation in expected count 0 variance of SMR is proportional to 1/6, c it is a saturated estimate of relative risk, and hence not parsimonious</p><ul><li>Thus, naive use of disease mapping on rare diseases or small areas can be very</li></ul><p>misleading (Molli, 1999)</p><p>0 Possibly, the most extreme SMRS are those based on only a few cases.</p><p>0 On the contrary, the most extreme p-values of tests comparing SMRs to unity or confidence intervals excluding unity may simply identify areas with large populations.</p><p>3.1.2 Spatial Smoothers</p><ul><li>Do we want to smooth the data? When interested in the expected values we</li></ul><p>might expect to do some smoothing.</p><ul><li><p>To smooth the observed quantity Y,- replace with 37 _ 211111ij 2 _ 101+</p></li><li><p>More generally, we could include the value actually observed for unit 73, and revise</p></li></ul><p>our smoother to</p><p>(1- 001/.- +0437.-</p><p>For 0 &lt; 04 &lt; 1, this is a linear (convex) combination of ’shrinkage‘ form.</p><ul><li><p>Finally, we could try model-based smoothing, i.e. based on the mean of the predictive distribution E(Y;|data). This leads to hierarchical spatial modeling</p></li><li>The basic idea is to “borrow” information from neighboring regions to produce a better estimate of the rate associated with each region</li><li><p>“Better” estimate means more stable and less noisy</p></li><li><p>In this way we separate out the “signal” (i.e., the spatial pattern) from the noise</p></li></ul><p>3.2 Hierarchical Bayesian Methods</p><ul><li><p>Think of the true underlying relative risks 6, as random effects, to allow ‘borrowing of strength’ across regions</p></li><li><p>Appropriate if we want to estimate and map the underlying risk surface</p></li><li><p>The random effects can be high dimensional, and are couched in a nonnormal (Poisson) likelihood. Thus, hierarchical Bayesian modeling seems natural.</p></li><li><p>Assume the following model Y,- N Poisson(E,6,-) 0 To circumvent shortcomings of the SMR as a relative risk estimator (extra-poisson variation), we want to control the behavior of the 6,- o This can be done by assuming that the parameters (the true risks) come from</p></li></ul><p>a common underlying distribution</p><p>o This also allow the procedure to ‘borrow strength’ across the various regions in order to come up with an improved estimate for the relative risk in each region.</p><ul><li>This is the idea of random effects and hierarchical modelling</li></ul><p>3.2.1 Poisson-Gamma Model</p><ul><li>Assume that the number of deaths is each area follow a Poisson distribution</li></ul><p>(likelihood) y, N Poisson(e,6i)</p><ul><li>Let the relative risks follow a gamma distribution (random effect, prior) 9,- N Gamma(a, b) with mean mg). : a/b and variance v91. : CL/b2</li><li>As a result, the relative risk has the following distribution (posterior) 9, N Gamma(a + yi, b + 6,)</li></ul><p>This emerges in closed form thanks to the conjugacy of the gamma prior with the</p><p>Poisson likelihood</p><p>Posterior Meag</p><ul><li>The posterior mean of the relative risk 67; is given by mg +</li></ul><p>a + yr 30– M</p><p>E 9. . : __- Z 4,.</p><p>[Zlyzi b+€1 1:24-61</p><p>92’ mg) I if W&quot; W U-Qi + 61’ z + 67; a,</p><ul><li>The posterior mean of 6,- is a weighted average of the data-based SMR for the ith area, and the relative risk in the overall map (the prior mean 77191.).</li></ul><p>Mg:</p><p>a+yz . 9 Eleilyil : m = CiSMRi+(1_ 02)]?</p><p>o for rare diseases and small areas</p><ul><li>0.; is small</li><li><p>posterior mean tends towards a global mean 61/!)</p></li><li><p>prior is highly informative 0 for areas with a lot of data</p></li><li><p>posterior mean is close to yi/ei</p></li><li><p>prior is weakly informative</p></li></ul><p>Example</p><ul><li>Suppose we have a Gamma(4,4) prior 0 In this case, we assume that = 1, and VarlQZ-l : 1/4</li><li>Suppose in area i we observe y,- : 27 cases, when e,- = 21 were expected, or risk: yi/e, = 1.29. 0 As a result, we obtain as posterior distribution</li></ul><p>Gamma(yi + a, e,- + b) = Gamma(31, 25)</p><p>o This distribution has mean 31/25 = 1.24 o This indicates slightly elevated risk (24%)</p><p>Example</p><p>Samples from a Gamma(4,4) prior Samples from a Gamma(31,25) posterior 8  8 0 e O 2 m 8 I X-I/dl % o 1 2 a 0.5 1.0 1.5 2.0 rgamma(1000, 4, 4) rgamma(1000, 31, 25)</p><p>Example</p><p>o The probability that the true risk is bigger than 1, is P(6 = 1|yZ) = 0.863</p><ul><li>This is derived exactly from the gamma distributionll</li><li>Can also be derived empirically as the proportion of samples that are greater than 1.</li></ul><p>o A 100(1 - a)% confidence interval for 6 can be derived by taking the upper and lower 04/2 points of the Gamma(31, 25) posterior</p><ul><li>This is called the equal-tail credibility interval</li><li>We obtain (0.84127 1.1713)</li></ul><p>Estimation of the model parameters</p><p>a and b in the Poisson-Gamma model are so-called hyperparameters.</p><p>0 Empirical Bayes approach</p><ul><li>hyperparameters are estimated from the data</li><li><p>yield acceptable point estimates of the rates</p></li><li><p>it underestimates their uncertainty 0 Full Bayesian approach</p></li><li><p>prior distributions for a and b are specified as well</p></li><li><p>this expresses our ignorance or prior knowledge about a and b</p></li><li><p>a N exponential(0.1) and b N exponential(0.1)</p></li><li><p>sensitivity analysis to investigate the influence of the choice of hyperprior on estimates of relative risk</p></li><li><p>if data are scarce, the choice of a suitable combination of hyperparameters is Important</p></li></ul><p>3.2.2 WinBugs Code: Poisson-Gamma model</p><p>model</p><p>{</p><p>for (i in 1 : N)</p><p>{ # Poisson likelihood for observed counts 0[i] ” dpoisCmU[i]) mu[i] &lt;- E[i]*theta[i]</p><h1 id="relative-risks"><span class="header-section-number">5</span> Relative Risks</h1><p>theta[i] ” dgamma(a, b) } # Vague prior distributions a ” deXp(0.01) b ” deXp(0.01)</p><h1 id="additional-estimates"><span class="header-section-number">6</span> Additional estimates</h1><p>m &lt;- a/b var &lt;- a/pow(b,2)</p><p>122</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>values for SMR<br />values for SMR I (5) &lt; 0.5 4 0.5 - I16 , lo I (a) [1.6- 0.3 ﬂ [1 (14) 0.8 - 1.2 ’ I (5) 1.2 - 1.5 I (5) 1.5 - 2.0 . I (3) &gt;= 2.0 2.00E+4km «if-H</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>(samples)means for theta 37 (samplesyneans for theta I (1) &lt; [1.5</p><p>1 .5 - .</p><p>N - ( J - o 6 I (12) 0.6- as ‘- (13) 0.3. 1.2’ I (10) 1.2 - 1.5</p><p>F I (2) 1.5- 2.0</p><p>IE I (0) &gt;= 2.0 2.UDE+4krn 4—b</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>Posterior Parameter Estimates:</p><p>node mean sd MC error 2.5% median 97.5% start sample a 3.57 0.9012 0.03898 2.062 3.489 5.59 10001 10000 b 3.495 0.9429 0.04048 1.903 3.408 5.595 10001 10000 mean 1.032 0.1062 0.00142 0.8409 1.025 1.26 10001 10000 var 0.3213 0.112 0.004055 0.1676 0.2998 0.6033 10001 10000</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>Posterior Estimates of Relative Risk:</p><p>node me theta[1] 1. theta[2] 1. theta[3] 0. theta[4] 0. theta[5] 1. theta[6] 1. theta[7] O. theta[8] O. theta[9] 1. theta[10] 1.</p><p>sd 0.3851 0.4519 0.1992 0.1914 0.3589 0.3786 0.217 0.2941 0.4053 0.1811</p><p>MC error</p><p>O O O O O O 0 0 0 0</p><p>.00438 .0052 .002209 .002385 .004094 .004608 .003479 .003245 .004673 .002093</p><p>2 O O O O 0 0 0 0 1 0</p><p>.5% .6168 .5546 .4992 .3043 .515 .6102 .1262 .4549 .146 .6924</p><p>median 1.214 1.221 0.8267 0.6029 1.053 1.192 0.4199 0.9187 1.828 1.003</p><p>97.570 2.118 2.318 1.286 1.048 1.921 2.076 0.951 1.61 2.747 1.398</p><p>.9512</p><p>start 10001 10001 10001 10001 10001 10001 10001 10001 10001 10001</p><p>sample 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000</p><p>m</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>caterpillar plot _ caterpillar plot: theta ‘ m m m m [a [61 __<em>.—</em>][81 __-.’– I29] [I no: “u [I31 [l4 [1151 “6] m m [I81 “9] m m] m] s m M [35] [:71 m [259’ I [3:]”Em [3+1 [3:1 _ ‘1 tab] __-.— m m] [331 m [u] [I- 1 .U 2.0 3.0 LI</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>ﬁprobabilittf of theta greater than Ll] I _</p><p>probability of theta greater than 1.0 - (3)4 035</p><ul><li>r?) 0.05- 0.15 N ‘I (14) 0.15. 0.5 I (13) 0.5. 0.?5 ‘ I (5) 0.?5- 0.95’ I (23&gt;: 0.95 2.00E+4km 4’–t</li></ul><p>m</p><p>Remarks</p><p>o Gamma prior is mathematically convenient</p><p>o Leads to robust estimates</p><p>0 But, covariate adjustment is difficult</p><p>0 Does not cope with spatial correlation between risks in nearby areas</p><p>o Extending this model to allow for such spatial correlations among the 67; also difficult (we would need a multivariate version of the gamma distribution)</p><p>0 As an alternative, use a multivariate normal distribution</p><p>3.2.3 Poisson-Lognormal Model</p><p>o The number of deaths is each area follow a Poisson distribution (likelihood)</p><p>yi N Poissomeidi)</p><p>0 Use normal prior distribution on the log-relative risks : 04 + ﬂiiﬁ + V1“ VZ’ N N(0,03) l&gt; VZ’ is the heterogeneity random effect, capturing extra-Poisson variability in the</p><p>log-relative risks - m are explanatory spatial covariates (at region-level), having parameter</p><p>coefficients 6</p><p>o Lognormal model for the relative risk is more flexible</p><p>0 Parameters VZ’ are called random effects</p><p>0 Vi represents the residual (unexplained) (log) relative risk in area 1’ after adjusting for known covariates and overall mean risk (a)</p><p>0 Vi captures the effects of unknown or unmeasured area level covariates</p><p>o The variance of the random effects (03) reflects the amount of extra-Poisson variation in the data.</p><p>3.2.4 WinBugs Code: Poisson-Lognormal model</p><p>model { for (i in 1 : N) {</p><h1 id="poisson-likelihood-for-observed-counts"><span class="header-section-number">7</span> Poisson likelihood for observed counts</h1><p>O[i] ” dpois(mu[i]) log(mu[i]) &lt;- log(E[i]) + alpha + H[i]</p><h1 id="heterogeneity-random-effects"><span class="header-section-number">8</span> Heterogeneity random effects</h1><p>H[i] ” dnorm(0, v.inv)</p><h1 id="relative-risks-1"><span class="header-section-number">9</span> relative risks</h1><p>thetaEi] &lt;- exp(alpha + H[i])</p><h1 id="vague-prior-distribution-for-intercept"><span class="header-section-number">10</span> Vague prior distribution for intercept</h1><p>alpha ” dnorm(0.0, 1.0E-5)</p><h1 id="hyperprior-distibutions-on-inverse-variance-parameter"><span class="header-section-number">11</span> Hyperprior distibutions on inverse variance parameter</h1><p>v.inv ” dgamma(0.01, 0.01) v &lt;- 1 / v.inv</p><p>132</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>values for SMR<br />values for SMR I (5) &lt; 0.5 4 0.5 - I16 N .0 I (3) 0.6- 0.3 ﬂ [1 (14) 0.8 - 1.2 ’ I (5) 1.2 - 1.5 I (5) 1.5 - 2.0 . I (3) &gt;= 2.0 2.00E+4km «if-H</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>N .03) 0.5- 0.6</p><p>l “4:, 5:13: “1</p><p>‘ Ito) 1.2- 1.5</p><p>&quot; (3) 1.5- 2.0</p><p>@‘g’ l<br />«£5.31 4%» 7</p><p>9“, am</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>Posterior Parameter Estimates:</p><p>node mean sd MC error 2.5% median 97.5% start sample alpha -0 0576 0.07584 0.00141 -0.2107 -0.05575 0.08602 10001 10000 tau.u 15.07 13.79 0.5335 4.491 11.8 46.5 10001 10000 var 0.09471 0.05194 0.001591 0.0215 0.08475 0.2228 10001 10000</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>probability of theta greater than Ll]</p><p>probability of theta greater than 1.0 - (0) &lt; 0.05</p><p>I: (5) 0.05- 0.15 N</p><p>I (22) 0.15. 0.5</p><ul><li><ol start="15" style="list-style-type: decimal"><li>0.5- 0.?5</li></ol></li></ul><p>. I (0) 0.?5- 0.05 I (2) &gt;= 0.95 2.DUE+4km +—I</p><p>M</p><p>3.2.5 Conditional autoregressive (CAR) model e</p><p>0 Many statistical methods assume that observations are independent</p><p>0 However, data that occur at locations close together in space are likely to be correlated</p><p>0 Dependence between observations is a realistic assumption</p><p>o Poisson-Gamma and Poisson-Lognormal model do not account for possible unknown spatial structures (e.g. environmental effects). These are called independent prior models.</p><p>0 Prior distributions should allow for spatial correlation (spatially structured priors)</p><p>Conditional autoregressive (CAR) model</p><p>l&gt; Gaussian (autonormal) model 1%.le 7s Z’) = N bz‘jymf j</p><ul><li>Using Brook’s Lemma we can obtain 1 _ 19(917 y27 ° ‘’ agn) OC exp 1(1 - where B : {big} and - is diagonal with DZ-Z- = 712</li><li>Suggests a multivariate normal distribution with ,uy : O and 2y : (I - B)-1D</li></ul><p>_ . . b~ b~ . . - - 1(I - B) symmetric requrres = for all 2,3 2 J</p><p>M</p><ul><li>Returning to W, let bz-j : wij/wpr and 7%? : T2/wZ-+, so</li></ul><p>1 19(917 y27 ‘’ ’ agn) QC exp -</p><p>where Du, is diagonal with : 10H and thus</p><p>1 M91, 292; - - - 73171) DC 6X10 -§;§szj(yi - yj)2 #J’ - This is the intrinsic autoregressive (IAR) model</p><ul><li><p>Improper distribution since (Dw - W)1 : 0, so requires a constraint, say :1 yz‘ : 0</p></li><li><p>Not a data model, a random effects model</p></li></ul><p>Proper CAR Model - Proper version: replace Du, - W by Dw - pW, and choose ,0 such that 2y : (Dw - [Owl-1 exists. This in turn implies</p><p>2 . i’Y’vT /ml) Yz’lngéz’ N N<prj J

J

- Advantages of using ,0:

- Makes distribution proper
- Adds parametric flexibility

- p = 0 interpretable as independence


Issues

- Disadvantages of using p:

- Why should we expect Y,- to be a proportion of average of neighbors - a
sensible spatial interpretation?

- Calibration of ,0 as a correlation, e.g.
- p = 0.80 yields 0.1 g Moran’s l g 0.15
- p = 0.90 yields 0.2 g Moran's l g 0.25

- p = 0.99 yields Moran's l g 0.5

- So, used with random effects, scope of spatial pattern may be limited


Bayesian Specification of CAR Model

o Introduced by Clayton and Kaldor (1987) in an empirical Bayes setting
0 Developed by Besag et al. (1991) in a fully Bayes implementation
0 Also called the Besag, York and Mollie model

o Widely used in the area of disease mapping!


Convolution Model

y, N P0isson(e,~«9,)
log(di) 2 oz + 5551+ “7; + 711

0 04 is an overall level of the relative risk

o v,- is the uncorrelated heterogeneity

v.- ~ No.05)

o u,- is the correlated heterogeneity

- a spatial correlation structure is used
- estimation of the risk in any area depends on neighboring areas

Iuilujai 7A jngl N Alva-“7012)

m

The conditional autregressive model uses:
- - 2 - 2
Iuiluj7 Z 7g JvTuI N  0i)

where

- _ 1 . ..
‘ Uz‘ - ijij Zj ujwlj
2

2 _ Uu
. 0-2' -- 

o Luz-j : 1 is adjacent (and 0 otherwise)

u,- is smoothed towards the mean risk in the set of neighboring areas, with variance
inversely proportional to the number of neighbors


Interpretation

/

o Area-specific random effects are decomposed into:
- clustering or correlated heterogeneity component
- uncorrelated heterogeneity component

0 of, and 0?, control the variability of u and v

o The model allows the data to decide how much of the residual disease risk is due
to spatially structured variation, and how much is unstructured over-dispersion.

- 03/03 small reflects an unstructured heterogeneity
- 03/012, large indicates that a spatially structured variation dominates
0 03 controls the strength of spatial similarity induced by the CAR prior

- small values of 0?, indicate stronger spatial correlation between neighboring

regions
- setting 03 = 0 produces complete shrinkage to a common value


3.2.6 WinBUGS Code: Convolution model

# Conditional-Autoregressive Model

model {

for (i in 1 :N) {
0[i] ” dpois(mu[i])
log(mu[i]) <- log(E[i]) + alpha + u[i] + v[i]
RR[i] <- eXp(alpha + u[i] + v[i])
v[i] ” dnorm(0,tau.v)

}

# CAR prior distribution for random effects:
u[l N] ” car.normal(adj[], weights[], num[], tau.u)
for(k in 1 sumNumNeigh) {

weights[k] <- 1

}

# Other priors:
alpha "dflat()
mean <- eXp(alpha)
tau u ~dgamma(0.5, 0.0005)


tau.v ~dgamma(0.5, 0.0005)


 N car.n0rmal(adj[], weightﬂ, numﬂ, tau.u)

o ade] is a vector listing the adjacent areas for each area; e.g.
adj : c(2,1,4,6,
4, 3,
8,3,1,2,5,6,
The adjacency matrix can be generated using the Adjacency Tool from the Map
menu.

0 weight[] is a vector of the same length as adjlj, giving weights associated with
each pair or areas; these weights can be generated using a loop in the WinBUGS

code

o  is a vector giving the number of neighbors for each area

num : C(4,2,6, 


a Two separate chains starting from different initial values

0 Convergence was checked by visual examination of time series plots of samples for
each chain

o Gelman and Rubin diagnostics were calculated
0 The first 20,000 samples were discarded as burn-in
0 Each chain was run for a further 10,000 iterations

0 Estimates of the relative risk show less variation than the observed SMR


Example 1: Cervix Cancer in Limburg, Belgium

values for SMR  
values for SMR I (5) < 0.5
4 0.5 - 0.0
, lo
I (3) 0.6- 0.3
ﬂ [1 (14) 0.8 - 1.2
' I (5) 1.2 - 1.5
I (5) 1.5 - 2.0
. I (3) >= 2.0 2.00E+4km «if-H</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>N =(0) 0.5- 0.3 i ‘5’ D::.°:.1°;: A? “ ’ii: 11:: :1: aw =i kiwi“? it ‘P , *‘D’. 3 w</p><p>151</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>(samples)means for u 3‘</p><p>(samples)rneansforu I (0)&lt; -0.3</p><p>4 -D.3- 43.2 N I0 l (13) -0.2- -0.1 I: (10) -0.1- 0.1 I (4) 0.1 - 0.2 d‘ I (2) 0.2- 0.3 ‘l I (4) 0.3 2.00E+4km 4–b</p><p>T 1 Ab dag,” ‘1’ 1gg4i&quot; , 4a 9“. N</p><ul><li>0.03</li></ul><p>152</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>Posterior Parameter Estimates:</p><p>node mean sd MC error 2.5% median 97.5% start sample</p><p>alpha -0.03222 0.06181 0.001151 -0.1577 -0.03046 0.08741 10001 10000 sigma.u 0.1844 0.1465 0.008371 0.001734 0.1595 0.537 10001 10000 Sigma.v 0.01105 0.02497 0.001945 1.785E-4 0.001787 0.09169 10001 1000&lt; tau.u 50.8 182.0 14.65 1.862 6.268 584.2 10001 10000</p><p>tau.v 1122.0 1529.0 97.37 10.94 559.8 5608.0 10001 10000</p><p>The variability of the relative risk is attributed less to the uncorrelated heterogeneity than to the spatially structured effects</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>I: . ‘ E I ’69:?» E «as ‘ t‘lg. “3%</p><p>:1ng</p><ol start="4" style="list-style-type: upper-alpha"><li>&lt; 0.05</li></ol><ol start="5" style="list-style-type: decimal"><li>0.05 - 0.15</li></ol><p>I (20) 0.15- 0.5 I (13) 0.5. 0.75 I (5) 0.?5- 0.95 I (13&gt;: 0.05</p><p>154</p><p>Example 1: Cervix Cancer in Limburg, Belgium</p><p>0 Which model is the best one?</p><p>0 Model comparison with DIC:</p><p>Dbar = post.mean of -21ogL; Dhat = -2LogL at post.mean of stochastj</p><p>MODEL Dbar Dhat p- DIC</p><p>PG 198.745 171.833 26.912 225.656 PL 206.355 189.152 17.203 223.558 CAR 208.222 194.186 14.036 222.258 CONV 206.613 191.743 14.870 221.483</p><p>o Convolution model is preferred</p><p>3.2.7 Other Correlation Models</p><p>l&gt; Other correlation models available in WinBugs: - Multivariate normal model (similar to Bayesian kriging): uses distances</p><p>between municipalities dij, e.g. u N Nn(0, 7’2) with EU- : (:0v(ui7 uj) = exp {-ozdij) - More flexible spatial form (2 parameters instead of 1)</p><ul><li><p>Stationary model</p></li><li><p>But: requires inversion of large n X n covariance matrix</p></li><li><p>Proper CAR model</p></li></ul><p>3.2.8 lmporting maps in WinBUGS m-</p><p>library(maptools) # loads Sp library too library(maps)</p><h1 id="read-in-shape-files-single-folder-with-all-associated-files-no"><span class="header-section-number">12</span> read in shape files (single folder with all associated files, no</h1><p>extension)</p><p>x&lt;-readShapePoly(“C:/folder/name.shp”)</p><h1 id="display-the-shapefile"><span class="header-section-number">13</span> display the shapefile</h1><p>plot(X)</p><h1 id="convert-the-shape-file-into-splus-format-to-import-into-geobugs"><span class="header-section-number">14</span> convert the shape file into splus format to import into GeoBugs</h1><p>sp2WB(X, “C:/folder/name.txt”) # note ’.txt’ extension</p><h1 id="open-winbugs-and-open-the-txt-file"><span class="header-section-number">15</span> Open WinBUGS and open the txt file</h1><h1 id="press-import-splus-and-save-the-.map-file-in-the-map-folder"><span class="header-section-number">16</span> Press Import SPlus and save the .map file in the map folder</h1><h1 id="restart-winbugs"><span class="header-section-number">17</span> Restart WinBUGS</h1><p>3.3 Ecological analysis</p><p>Example 2: South Africa Unemployment Data</p><ul><li>Census Data in South Africa: 2001,2011</li><li>Number of households with access to piped water</li><li><p>% of 20+ year-olds without any schooling</p></li><li><p>Around 5% of population sampled</p></li><li><p>Is the number of households with access to piped water related to the education?</p></li></ul><p>South Africa Neighborhood Structure</p><p>‘-~’4}&lt;§ « ..4’('§M‘vebse€a .</p><p>.v.’!¢m§vﬁa§= c. . ‘:2&quot; ‘- v’</p><p>4gA¢‘V)§§‘§aAVSw’ c?» 3 a</p><p>M</p><p>Proportion of households with access to piped water Proportion Piped Water (2001)</p><p>1:: [1.6,31.425)</p><p>r3 [31.425,63.5) m</p><p>:1 [63.5,79.65)</p><p>III [7965,9591 90“” .a</p><p>‘ ’t‘gsby * tw ﬁlm-w $3 , . “lg‘ﬁsy a or 3”&quot; ééwéﬁ&quot;</p><p>water.jpg-g’</p><p>Proportion of 20+ year-olds without schooling Proportion No Schooling (2001)</p><p>Cl [4.1422 15 Ell . [3.3.1.3133-36043 ’ ,67.9317] v&quot; y a ‘W? ‘i . *nw’e’a4‘ li’wv 3““ cl twﬁvﬁ“ ‘v d-t&quot; #3”! tray Q‘V‘” v ofﬁw‘é?’ A“ *3 “aﬁgﬁ éﬁQ-ﬂ‘lﬁ ﬁﬁﬁvﬁxrl 725$</p><p>schoolingjpg</p><p>3.3.1 Binomial Model m</p><p>This can be modelled using a binomial likelihood:</p><p>yz. N PoissoanPi) logitm) = 04 i 55’” + “i + M</p><p>0 04 is an overall level of the relative risk 0 ﬂ is the covariate effect</p><p>o ’07; is the uncorrelated heterogeneity 2 ’02- N N(0, 0U)</p><p>o ui is the correlated heterogeneity</p><ul><li><p>a spatial correlation structure is used</p></li><li><p>estimation of the risk in any area depends on neighboring areas</p></li></ul><p>mum #93731 ~ N (at, a?)</p><p>M</p><p>3.3.2 Code in WinBugs</p><p>model</p><p>{ for (i in 1 :N) { PW[i] &quot; dbin(prop[i],POP[i]) logit(prop[i]) &lt;- alpha + beta*(school[i]-school.bar)</p><ul><li>u[i] + v[i]</li></ul><h1 id="area-specific-random-effects"><span class="header-section-number">18</span> area-specific random effects:</h1><p>v[i] ” dnorm(0, tau.v)</p><h1 id="residual"><span class="header-section-number">19</span> residual</h1><p>res[i]&lt;-prop[i]-(PW[i]/POP[i])</p><h1 id="car-prior-distribution-for-random-effects"><span class="header-section-number">20</span> CAR prior distribution for random effects:</h1><p>u[1 N] ” car normal(adj<a href="#section"></a>, weights<a href="#section"></a>, num<a href="#section"></a>, tau.u)</p><p>for(k in 1 sumNumNeigh) {</p><p>weights[k] &lt;- 1 }</p><h1 id="other-priors"><span class="header-section-number">21</span> Other priors:</h1><p>alpha ~dflat() beta ” dnorm(0,tau.b)</p><p>tau.u ~dgamma(0.001, 0.001) # prior on precision tau.v ~dgamma(0.001, 0.001) # prior on precision tau b ~dgamma(0.001, 0.001) # prior on precision</p><p>school.bar &lt;- mean(school<a href="#section"></a>)</p><p>Predicted Proportions</p><p>Predicted Proportion Piped Waterd Standard Deviation of Prediction</p><p>El [0.02177,0.316975) El [0.316975,0.63275) ~ﬂ El [0.002777,0.0121175) 3mm gage f &quot; at 3‘4 ivy-wt ,! [Messrs ;. wages” M fag? , ‘I’ﬁxgxwi t. Utiﬁwté ‘ t W a; <span class="citation">@ay</span>’ “‘o. .’ :3 Wm” } «we? aéeemw earma- n; )-</p><p>op-J’pg prop-jpg</p><p>W</p><p>Unstructured Heterogeneity Terms</p><p>[:1 [-0.774,-0.0731175)</p><p>El [-0.0731175,0.0089865) ~’</p><p>3 i8:823??$§:81233;i75’ 1% vs?</p><p>.Wﬁ’ﬁaﬁﬁii ﬁght,“ a? ﬁt»??? #3 «u %mew a, 4 ‘v&quot; @5535?!”</p><p>M</p><p>W</p><p>Spatially Structured Heterogeneity Terms</p><p>I: [-2.342,-0.5384) - [-0.5384,0.1443) 4.7, a[3-;:::.0-.6:§:5&gt; “t” [. ,. 1 ‘m aw.“ «- Qvnrﬁairﬁél’y‘ 4s bﬁsrﬁsus p</p><p>Va ’1- 7‘ v ﬁat-$9“</p><p>ls % no-schooling significant?</p><p>ﬁKernel density [a beta sample: 50000 ‘ 0.8 0.5 0.4 0.2 0.0 -10.0 -B.0 -6.0 :1</p><p>node mean sd LL median UL start sample alpha 0.2169000 0.021360 0.172900 0.216900 0.260800 10001 60000 beta -7.1140000 0.589400 -8.305000 -7.121000 -5.889000 10001 60000</p><p>An effect at the aggregate level cannot be interpreted on the individual level: ecological fallacy!</p><p>WE</p><p>Residuals</p><p>El [-0.938,-0.0108075) 1:! -o. 7 ,- . 012 85 .r amagiéizafszi Q‘ogtgay‘W‘ <span class="math">\(¢rﬁwﬁxs\)</span>~w hﬁiwﬁmaw</p><p>‘5 Vﬂuﬁ’tﬁs‘?‘ n: ~ AIM;</p>
